from crewai import Agent, Task, Crew
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import os
import requests
import re
from sheets_utils import (
    send_to_google_sheet, 
    get_existing_entries, 
    get_keywords_from_sheet, 
    generate_crew_prompt, 
    parse_crew_output,
    test_google_sheets_connection
)

# Charger les variables d'environnement (.env)
load_dotenv()

# Test de connexion Google Sheets au d√©marrage
print("üîß V√©rification de la connexion Google Sheets...")
if not test_google_sheets_connection():
    print("‚ùå Impossible de se connecter √† Google Sheets. V√©rifiez votre fichier credentials.json")
    exit(1)

# Initialiser le mod√®le LLM
llm = ChatOpenAI(model="gpt-4-turbo")

# G√©n√©rer dynamiquement le prompt bas√© sur les colonnes du Google Sheet
prompt_text, expected_headers = generate_crew_prompt()
print(f"\nüìã Colonnes √† rechercher : {expected_headers}\n")

# R√©cup√©rer les aides d√©j√† trouv√©es
existing_aides = get_existing_entries()

# Agent 1 : Recherche
research_agent = Agent(
    role="Chercheur d'aides au documentaire",
    goal="Identifier et extraire des aides financi√®res pertinentes pour un documentaire en postproduction, abordant l'animisme et les esprits, tourn√© en Tha√Ølande et coproduit avec la France.",
    backstory="Expert en financement culturel pour documentaires internationaux.",
    verbose=True,
    llm=llm
)

# Agent 2 : Nettoyeur
data_cleaning_agent = Agent(
    role="Nettoyeur de donn√©es",
    goal="Nettoyer et uniformiser les informations collect√©es pour cr√©er une base exploitable, en respectant exactement les colonnes demand√©es.",
    backstory="Sp√©cialiste de la normalisation de donn√©es pour des bases structur√©es.",
    verbose=True,
    llm=llm
)

# Agent 3 : V√©rificateur & Analyste
analysis_agent = Agent(
    role="V√©rificateur et analyste strat√©gique",
    goal="V√©rifier la pertinence des liens, s'assurer qu'ils pointent vers des aides sp√©cifiques, et enrichir avec des commentaires strat√©giques.",
    backstory="Consultant expert en montage de dossiers de financement pour films internationaux.",
    verbose=True,
    llm=llm
)

# G√©n√©ration de la consigne en excluant les aides d√©j√† connues
exclusion_text = ""
if existing_aides:
    # Extraire les noms des aides existantes
    existing_names = []
    for aide in existing_aides:
        # Chercher le champ nom dans diff√©rentes variations possibles
        nom = aide.get('Nom') or aide.get('nom') or aide.get('NAME') or ""
        if nom:
            existing_names.append(nom)
    
    if existing_names:
        exclusion_text = "\nIgnore les aides d√©j√† list√©es avec les noms suivants :\n" + "\n".join(
            f"- {nom}" for nom in existing_names
        )

# API Google Search params
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CX = os.getenv("GOOGLE_CSE_ID")  # Correction du nom de la variable

# Charger dynamiquement les mots-cl√©s depuis Google Sheets (onglet "MotsCl√©s")
keywords_to_test = get_keywords_from_sheet()

# Si pas de mots-cl√©s dans le sheet, utiliser des mots-cl√©s par d√©faut
if not keywords_to_test:
    print("‚ö†Ô∏è Aucun mot-cl√© trouv√© dans l'onglet 'MotsCl√©s'. Utilisation des mots-cl√©s par d√©faut.")
    keywords_to_test = [
        "aide documentaire postproduction France",
        "financement documentaire coproduction internationale",
        "subvention documentaire culturel 2024"
    ]

print(f"\nüîç Mots-cl√©s √† rechercher : {keywords_to_test}\n")

# API perso pour extraire le contenu des pages
CONTENT_API_KEY = os.getenv("VERIFYBOT_CONTENT_API_KEY")

def google_search_urls(query):
    """Effectue une recherche Google et retourne les URLs"""
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": GOOGLE_API_KEY,
        "cx": GOOGLE_CX,
        "q": query
    }
    try:
        res = requests.get(url, params=params)
        results = res.json()
        links = [item["link"] for item in results.get("items", [])][:5]  # Limiter √† 5 r√©sultats
        print(f"\nüîç Recherche : {query}")
        for link in links:
            print(f"  - {link}")
        return links
    except Exception as e:
        print(f"‚ùå Erreur recherche Google : {e}")
        return []

def get_page_content(target_url):
    """Extrait le contenu d'une page web"""
    api_url = f"https://cockpit.verifybot.app/api-get-content.php"
    params = {
        "url": target_url,
        "key": CONTENT_API_KEY
    }
    
    # Log de l'URL pour debug
    print(f"  üì° Appel API : {api_url}?url={target_url}&key={'*' * 10 if CONTENT_API_KEY else 'NO_KEY'}")
    
    try:
        response = requests.get(api_url, params=params, timeout=10)
        data = response.json()
        
        if response.status_code != 200:
            print(f"  ‚ùå Erreur HTTP {response.status_code}")
            return None
            
        content = data.get("content", "")
        if content:
            print(f"  ‚úÖ Contenu extrait : {len(content)} caract√®res")
        else:
            print(f"  ‚ö†Ô∏è R√©ponse vide ou erreur : {data.get('error', 'Aucun contenu')}")
            
        return content if content else None
    except Exception as e:
        print(f"  ‚ùå Exception : {e}")
        return None

# Collecter le contenu des pages
documents_text = ""
total_urls = 0

for keyword in keywords_to_test:
    urls = google_search_urls(keyword)
    for url in urls:
        try:
            content = get_page_content(url)
            if content:
                documents_text += f"\n\n---\nContenu extrait de : {url}\n{content[:5000]}\n"  # Limiter la taille
                total_urls += 1
            else:
                print(f"‚ö†Ô∏è Aucun contenu extrait pour : {url}")
        except Exception as e:
            print(f"Erreur sur {url}: {e}")

print(f"\nüìö Total : {total_urls} pages extraites\n")

# Si aucun contenu trouv√©, arr√™ter
if not documents_text:
    print("‚ùå Aucun contenu trouv√©. V√©rifiez vos cl√©s API.")
    exit(1)

# T√¢che de recherche avec prompt dynamique
funding_task = Task(
    description=f"""{prompt_text}
    
    IMPORTANT : Pour chaque aide trouv√©e, extrais TOUTES les informations demand√©es.
    Si une information n'est pas disponible, indique "Non sp√©cifi√©" mais inclus quand m√™me le champ.
    
    {exclusion_text}
    
    Contenu √† analyser :
    {documents_text[:50000]}""",  # Limiter la taille pour GPT
    expected_output=f"Une liste structur√©e d'aides avec EXACTEMENT ces champs : {', '.join(expected_headers)}",
    agent=research_agent
)

# T√¢che de nettoyage
data_cleaning_task = Task(
    description=f"""Prends les r√©sultats et nettoie-les pour un tableur :
    - Supprime TOUS les caract√®res de formatage : *, **, _, __, #, ##, ###, etc.
    - Supprime les retours √† la ligne multiples et remplace par des espaces
    - Supprime les tabulations et caract√®res sp√©ciaux
    - Convertis tout en texte brut, sans formatage markdown ou HTML
    - Assure-toi que chaque aide a TOUS les champs suivants : {', '.join(expected_headers)}
    - Standardise les formats (dates en DD/MM/YYYY, emails sans espaces, liens complets avec https://)
    - Garde un format coh√©rent pour chaque entr√©e
    - Maximum 500 caract√®res par champ pour √©viter les d√©bordements
    - Remplace les caract√®res probl√©matiques comme les guillemets par des apostrophes simples""",
    expected_output=f"Liste propre en texte brut avec ces champs exacts : {', '.join(expected_headers)}",
    agent=data_cleaning_agent
)

# T√¢che d'analyse
analysis_task = Task(
    description=f"""V√©rifie et enrichis chaque aide :
    - V√©rifie que les liens sont pertinents (pas de pages d'accueil g√©n√©riques)
    - Ajoute des commentaires strat√©giques sur l'ad√©quation avec le projet
    - Compl√®te les informations manquantes si possible
    - Structure finale avec TOUS ces champs : {', '.join(expected_headers)}""",
    expected_output=f"Version finale enrichie avec tous les champs : {', '.join(expected_headers)}",
    agent=analysis_agent
)

# Cr√©ation de la crew
crew = Crew(
    agents=[research_agent, data_cleaning_agent, analysis_agent],
    tasks=[funding_task, data_cleaning_task, analysis_task],
    verbose=True
)

print("\nüöÄ Lancement de la recherche d'aides...\n")

# Ex√©cution
try:
    result = crew.kickoff()
    result_text = str(result)
    
    print("\nüìÑ R√©sultat brut (aper√ßu) :")
    print(result_text[:1000] + "..." if len(result_text) > 1000 else result_text)
    
    # Parser le r√©sultat avec la nouvelle fonction dynamique
    entries = parse_crew_output(result_text, expected_headers)
    
    print(f"\nüìä {len(entries)} aide(s) extraite(s)")
    
    # Si pas d'entr√©es, essayer un parsing alternatif
    if not entries:
        print("\n‚ö†Ô∏è Parsing standard √©chou√©. Tentative de parsing alternatif...")
        
        # M√©thode alternative : chercher des blocs de texte structur√©s
        # Chercher toutes les URLs dans le texte
        urls = re.findall(r'https?://[^\s]+', result_text)
        print(f"URLs trouv√©es dans le r√©sultat : {len(urls)}")
        
        # Cr√©er des entr√©es basiques avec ce qu'on trouve
        for i, url in enumerate(urls[:10]):  # Limiter √† 10
            # Chercher du contexte autour de l'URL
            url_context = ""
            url_pos = result_text.find(url)
            if url_pos > 0:
                # Prendre 200 caract√®res avant et apr√®s l'URL
                start = max(0, url_pos - 200)
                end = min(len(result_text), url_pos + len(url) + 200)
                url_context = result_text[start:end]
            
            # Essayer d'extraire un nom
            nom_patterns = [
                r'(?:Nom|Aide|Programme|Fonds)\s*:\s*([^\n]+)',
                r'(?:^|\n)([A-Z][^:\n]{10,50})(?=\n)',
                r'(?:aide|subvention|financement)\s+([^\n]+)'
            ]
            
            nom = f"Aide {i+1}"  # Nom par d√©faut
            for pattern in nom_patterns:
                match = re.search(pattern, url_context, re.IGNORECASE)
                if match:
                    nom = match.group(1).strip()
                    break
            
            # Cr√©er une entr√©e basique
            entry = {}
            
            # Remplir avec les colonnes attendues
            for header in expected_headers:
                if 'nom' in header.lower():
                    entry[header] = nom
                elif 'lien' in header.lower() or 'url' in header.lower():
                    entry[header] = url.strip()
                elif 'r√©sum√©' in header.lower() or 'resume' in header.lower():
                    entry[header] = url_context.replace('\n', ' ').strip()[:200]
                elif 'statut' in header.lower():
                    entry[header] = "√Ä v√©rifier"
                elif 'organisme' in header.lower():
                    if 'cnc' in url.lower():
                        entry[header] = "CNC"
                    elif 'scam' in url.lower():
                        entry[header] = "SCAM"
                    elif 'iledefrance' in url.lower():
                        entry[header] = "R√©gion √éle-de-France"
                    else:
                        entry[header] = ""
                elif 'pays' in header.lower():
                    if any(keyword in url.lower() for keyword in ['cnc', 'scam', 'iledefrance', 'france']):
                        entry[header] = "France"
                    else:
                        entry[header] = ""
                else:
                    entry[header] = ""
            
            entries.append(entry)
        
        print(f"\nüìä {len(entries)} aide(s) cr√©√©e(s) par parsing alternatif")
    
    if entries:
        print("\nüîç Aper√ßu des entr√©es extraites :")
        for i, entry in enumerate(entries[:3]):
            print(f"\n--- Entr√©e {i+1} ---")
            for key, value in entry.items():
                print(f"  {key}: {value[:100] if value and len(str(value)) > 100 else value}")
        
        # Envoi vers Google Sheets
        print("\nüì§ Envoi vers Google Sheets...")
        send_to_google_sheet(entries)
    else:
        print("\n‚ùå Aucune aide trouv√©e m√™me avec le parsing alternatif")
        print("\nD√©but du r√©sultat brut pour analyse :")
        print(result_text[:1000])
        
except Exception as e:
    print(f"\n‚ùå Erreur lors de l'ex√©cution : {e}")
    import traceback
    traceback.print_exc()

#print("\n‚úÖ Script termin√©")

# Remplacer le dernier "print("\n‚úÖ Script termin√©")" par :

# Email de notification
try:
    from tools.smtp_email_tool import smtp_email_sender

    # Pr√©parer le message
    if entries:
        subject = f"‚úÖ Funding Script - {len(entries)} nouvelles aides"
        message = f"Script termin√© avec succ√®s. {len(entries)} nouvelles aides ajout√©es au Google Sheet."
    else:
        subject = "‚ö†Ô∏è Funding Script - Aucune nouvelle aide"
        message = "Script termin√© mais aucune nouvelle aide trouv√©e."

    # Ajouter timestamp
    # message += f"\nEx√©cut√© le {datetime.now().strftime('%d/%m/%Y √† %H:%M')}"

    # Envoyer
    email_result = smtp_email_sender.invoke({
        "subject": subject,
        "content": message
    })
    print(f"\nüìß Email envoy√© : {email_result}")

except Exception as e:
    print(f"\n‚ö†Ô∏è Erreur envoi email : {e}")

print("\n‚úÖ Script termin√©")
