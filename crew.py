from crewai import Agent, Task, Crew
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import os
import requests
from sheets_utils import send_to_google_sheet, get_existing_entries, get_keywords_from_sheet, generate_crew_prompt, parse_crew_output

# Charger les variables d'environnement (.env)
load_dotenv()

# Initialiser le modÃ¨le LLM
llm = ChatOpenAI(model="gpt-4-turbo")

# GÃ©nÃ©rer dynamiquement le prompt basÃ© sur les colonnes du Google Sheet
prompt_text, expected_headers = generate_crew_prompt()
print(f"\nğŸ“‹ Colonnes Ã  rechercher : {expected_headers}\n")

# RÃ©cupÃ©rer les aides dÃ©jÃ  trouvÃ©es
existing_aides = get_existing_entries()

# Agent 1 : Recherche
research_agent = Agent(
    role="Chercheur d'aides au documentaire",
    goal="Identifier et extraire des aides financiÃ¨res pertinentes pour un documentaire en postproduction, abordant l'animisme et les esprits, tournÃ© en ThaÃ¯lande et coproduit avec la France.",
    backstory="Expert en financement culturel pour documentaires internationaux.",
    verbose=True,
    llm=llm
)

# Agent 2 : Nettoyeur
data_cleaning_agent = Agent(
    role="Nettoyeur de donnÃ©es",
    goal="Nettoyer et uniformiser les informations collectÃ©es pour crÃ©er une base exploitable, en respectant exactement les colonnes demandÃ©es.",
    backstory="SpÃ©cialiste de la normalisation de donnÃ©es pour des bases structurÃ©es.",
    verbose=True,
    llm=llm
)

# Agent 3 : VÃ©rificateur & Analyste
analysis_agent = Agent(
    role="VÃ©rificateur et analyste stratÃ©gique",
    goal="VÃ©rifier la pertinence des liens, s'assurer qu'ils pointent vers des aides spÃ©cifiques, et enrichir avec des commentaires stratÃ©giques.",
    backstory="Consultant expert en montage de dossiers de financement pour films internationaux.",
    verbose=True,
    llm=llm
)

# GÃ©nÃ©ration de la consigne en excluant les aides dÃ©jÃ  connues
exclusion_text = ""
if existing_aides:
    # Extraire les noms des aides existantes
    existing_names = []
    for aide in existing_aides:
        # Chercher le champ nom dans diffÃ©rentes variations possibles
        nom = aide.get('Nom') or aide.get('nom') or aide.get('NAME') or ""
        if nom:
            existing_names.append(nom)
    
    if existing_names:
        exclusion_text = "\nIgnore les aides dÃ©jÃ  listÃ©es avec les noms suivants :\n" + "\n".join(
            f"- {nom}" for nom in existing_names
        )

# API Google Search params
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CX = os.getenv("GOOGLE_CSE_ID")  # Correction du nom de la variable

# Charger dynamiquement les mots-clÃ©s depuis Google Sheets (onglet "MotsClÃ©s")
keywords_to_test = get_keywords_from_sheet()

# Si pas de mots-clÃ©s dans le sheet, utiliser des mots-clÃ©s par dÃ©faut
if not keywords_to_test:
    print("âš ï¸ Aucun mot-clÃ© trouvÃ© dans l'onglet 'MotsClÃ©s'. Utilisation des mots-clÃ©s par dÃ©faut.")
    keywords_to_test = [
        "aide documentaire postproduction France",
        "financement documentaire coproduction internationale",
        "subvention documentaire culturel 2024"
    ]

# Limiter le nombre de mots-clÃ©s pour les tests (enlever cette ligne pour tout traiter)
# keywords_to_test = keywords_to_test[:3]

print(f"\nğŸ” Mots-clÃ©s Ã  rechercher : {keywords_to_test}\n")

# API perso pour extraire le contenu des pages
CONTENT_API_KEY = os.getenv("VERIFYBOT_CONTENT_API_KEY")

def google_search_urls(query):
    """Effectue une recherche Google et retourne les URLs"""
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": GOOGLE_API_KEY,
        "cx": GOOGLE_CX,
        "q": query
    }
    try:
        res = requests.get(url, params=params)
        results = res.json()
        links = [item["link"] for item in results.get("items", [])][:5]  # Limiter Ã  5 rÃ©sultats
        print(f"\nğŸ” Recherche : {query}")
        for link in links:
            print(f"  - {link}")
        return links
    except Exception as e:
        print(f"âŒ Erreur recherche Google : {e}")
        return []

def get_page_content(target_url):
    """Extrait le contenu d'une page web"""
    api_url = f"https://cockpit.verifybot.app/api-get-content.php"
    params = {
        "url": target_url,
        "key": CONTENT_API_KEY
    }
    
    # Log de l'URL pour debug
    print(f"  ğŸ“¡ Appel API : {api_url}?url={target_url}&key={'*' * 10 if CONTENT_API_KEY else 'NO_KEY'}")
    
    try:
        response = requests.get(api_url, params=params, timeout=10)
        data = response.json()
        
        if response.status_code != 200:
            print(f"  âŒ Erreur HTTP {response.status_code}")
            return None
            
        content = data.get("content", "")
        if content:
            print(f"  âœ… Contenu extrait : {len(content)} caractÃ¨res")
        else:
            print(f"  âš ï¸ RÃ©ponse vide ou erreur : {data.get('error', 'Aucun contenu')}")
            
        return content if content else None
    except Exception as e:
        print(f"  âŒ Exception : {e}")
        return None

# Collecter le contenu des pages
documents_text = ""
total_urls = 0

for keyword in keywords_to_test:
    urls = google_search_urls(keyword)
    for url in urls:
        try:
            content = get_page_content(url)
            if content:
                documents_text += f"\n\n---\nContenu extrait de : {url}\n{content[:5000]}\n"  # Limiter la taille
                total_urls += 1
            else:
                print(f"âš ï¸ Aucun contenu extrait pour : {url}")
        except Exception as e:
            print(f"Erreur sur {url}: {e}")

print(f"\nğŸ“š Total : {total_urls} pages extraites\n")

# Si aucun contenu trouvÃ©, arrÃªter
if not documents_text:
    print("âŒ Aucun contenu trouvÃ©. VÃ©rifiez vos clÃ©s API.")
    exit(1)

# TÃ¢che de recherche avec prompt dynamique
funding_task = Task(
    description=f"""{prompt_text}
    
    IMPORTANT : Pour chaque aide trouvÃ©e, extrais TOUTES les informations demandÃ©es.
    Si une information n'est pas disponible, indique "Non spÃ©cifiÃ©" mais inclus quand mÃªme le champ.
    
    {exclusion_text}
    
    Contenu Ã  analyser :
    {documents_text[:50000]}""",  # Limiter la taille pour GPT
    expected_output=f"Une liste structurÃ©e d'aides avec EXACTEMENT ces champs : {', '.join(expected_headers)}",
    agent=research_agent
)

# TÃ¢che de nettoyage
data_cleaning_task = Task(
    description=f"""Prends les rÃ©sultats et nettoie-les :
    - Supprime tous les caractÃ¨res de formatage markdown
    - Assure-toi que chaque aide a TOUS les champs suivants : {', '.join(expected_headers)}
    - Standardise les formats (dates, emails, liens)
    - Garde un format cohÃ©rent pour chaque entrÃ©e""",
    expected_output=f"Liste propre avec ces champs exacts : {', '.join(expected_headers)}",
    agent=data_cleaning_agent
)

# TÃ¢che d'analyse
analysis_task = Task(
    description=f"""VÃ©rifie et enrichis chaque aide :
    - VÃ©rifie que les liens sont pertinents (pas de pages d'accueil gÃ©nÃ©riques)
    - Ajoute des commentaires stratÃ©giques sur l'adÃ©quation avec le projet
    - ComplÃ¨te les informations manquantes si possible
    - Structure finale avec TOUS ces champs : {', '.join(expected_headers)}""",
    expected_output=f"Version finale enrichie avec tous les champs : {', '.join(expected_headers)}",
    agent=analysis_agent
)

# CrÃ©ation de la crew
crew = Crew(
    agents=[research_agent, data_cleaning_agent, analysis_agent],
    tasks=[funding_task, data_cleaning_task, analysis_task],
    verbose=True
)

print("\nğŸš€ Lancement de la recherche d'aides...\n")

# ExÃ©cution
try:
    result = crew.kickoff()
    result_text = str(result)
    
    print("\nğŸ“„ RÃ©sultat brut (aperÃ§u) :")
    print(result_text[:1000] + "..." if len(result_text) > 1000 else result_text)
    
    # Parser le rÃ©sultat avec la nouvelle fonction dynamique
    entries = parse_crew_output(result_text, expected_headers)
    
    print(f"\nğŸ“Š {len(entries)} aide(s) extraite(s)")
    
    # Si pas d'entrÃ©es, essayer un parsing alternatif
    if not entries:
        print("\nâš ï¸ Parsing standard Ã©chouÃ©. Tentative de parsing alternatif...")
        
        # MÃ©thode alternative : chercher des blocs de texte structurÃ©s
        import re
        
        # Chercher toutes les URLs dans le texte
        urls = re.findall(r'https?://[^\s]+', result_text)
        print(f"URLs trouvÃ©es dans le rÃ©sultat : {len(urls)}")
        
        # CrÃ©er des entrÃ©es basiques avec ce qu'on trouve
        for i, url in enumerate(urls[:10]):  # Limiter Ã  10
            # Chercher du contexte autour de l'URL
            url_context = ""
            url_pos = result_text.find(url)
            if url_pos > 0:
                # Prendre 200 caractÃ¨res avant et aprÃ¨s l'URL
                start = max(0, url_pos - 200)
                end = min(len(result_text), url_pos + len(url) + 200)
                url_context = result_text[start:end]
            
            # Essayer d'extraire un nom
            nom_patterns = [
                r'(?:Nom|Aide|Programme|Fonds)\s*:\s*([^\n]+)',
                r'(?:^|\n)([A-Z][^:\n]{10,50})(?=\n)',
                r'(?:aide|subvention|financement)\s+([^\n]+)'
            ]
            
            nom = f"Aide {i+1}"  # Nom par dÃ©faut
            for pattern in nom_patterns:
                match = re.search(pattern, url_context, re.IGNORECASE)
                if match:
                    nom = match.group(1).strip()
                    break
            
            # CrÃ©er une entrÃ©e basique
            entry = {
                "Nom": nom,
                "Lien": url.strip(),
                "RÃ©sumÃ©": url_context.replace('\n', ' ').strip()[:200],
                "Statut": "Ã€ vÃ©rifier"
            }
            
            # Essayer d'extraire d'autres infos
            if 'cnc' in url.lower():
                entry["Organisme"] = "CNC"
                entry["Pays"] = "France"
            elif 'scam' in url.lower():
                entry["Organisme"] = "SCAM"
                entry["Pays"] = "France"
            elif 'iledefrance' in url.lower():
                entry["Organisme"] = "RÃ©gion Ãle-de-France"
                entry["Pays"] = "France"
            
            entries.append(entry)
        
        print(f"\nğŸ“Š {len(entries)} aide(s) crÃ©Ã©e(s) par parsing alternatif")
    
    if entries:
        print("\nğŸ” AperÃ§u des entrÃ©es extraites :")
        for i, entry in enumerate(entries[:3]):
            print(f"\n--- EntrÃ©e {i+1} ---")
            for key, value in entry.items():
                print(f"  {key}: {value[:100] if value and len(value) > 100 else value}")
        
        # Envoi vers Google Sheets
        print("\nğŸ“¤ Envoi vers Google Sheets...")
        send_to_google_sheet(entries)
    else:
        print("\nâŒ Aucune aide trouvÃ©e mÃªme avec le parsing alternatif")
        print("\nDÃ©but du rÃ©sultat brut pour analyse :")
        print(result_text[:1000])
    else:
        print("\nâš ï¸ Aucune aide trouvÃ©e dans le rÃ©sultat. VÃ©rifiez le format de sortie des agents.")
        
        # Mode debug : essayer un parsing alternatif
        print("\nğŸ”§ Tentative de parsing alternatif...")
        
        # Rechercher des patterns simples
        simple_pattern = r"Nom\s*:\s*(.+?)(?:\n|$)"
        matches = re.findall(simple_pattern, result_text, re.MULTILINE)
        if matches:
            print(f"TrouvÃ© {len(matches)} nom(s) d'aide : {matches[:3]}...")
        
except Exception as e:
    print(f"\nâŒ Erreur lors de l'exÃ©cution : {e}")
    import traceback
    traceback.print_exc()

print("\nâœ… Script terminÃ©")
